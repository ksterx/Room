{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from room.envs import GymEnvWrapper\n",
    "from typing import List, Optional, Union, Dict\n",
    "from room.common.typing import CfgType\n",
    "import torch\n",
    "from tqdm import trange\n",
    "\n",
    "from room import notice\n",
    "from room.agents import Agent, DQN\n",
    "from room.common.callbacks import Callback\n",
    "from room.common.utils import get_param\n",
    "from room.envs.wrappers import EnvWrapper\n",
    "from room.loggers import Logger\n",
    "from room.memories import Memory, RandomMemory\n",
    "from room.trainers.base import Trainer\n",
    "from room.agents.policies import Policy\n",
    "from room.networks.blocks import MLP\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env = GymEnvWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DQN(Agent):\n",
    "    \n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         policy: Union[Policy, str],\n",
    "#         cfg: CfgType,\n",
    "#         logger: Optional[Logger] = None,\n",
    "#         *args,\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "#         super().__init__(policy, cfg, logger)\n",
    "#         self.policy = MLP([4, 100, 2], activation=\"relu\")\n",
    "#         self.q_net = self.policy\n",
    "#         self.target_q_net = self.policy\n",
    "#         self.loss_fn = torch.nn.functional.smooth_l1_loss\n",
    "\n",
    "#     def act(self, state: torch.Tensor) -> torch.Tensor:\n",
    "#         q = self.target_q_net(state)\n",
    "#         action = torch.argmax(q)\n",
    "\n",
    "#         # eposilon greedy\n",
    "#         if torch.rand(1) < 0.1:\n",
    "#             action = torch.randint(0, 2, (1,))\n",
    "#         return action\n",
    "\n",
    "#     def learn(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "#         q = self.q_net(batch[\"state\"])\n",
    "#         q = q.gather(1, batch[\"action\"].unsqueeze(0))\n",
    "#         q_next = self.target_q_net(batch[\"next_state\"])\n",
    "#         q_next = q_next.max(1)[0]\n",
    "#         q_target = batch[\"reward\"] + 0.01 * q_next\n",
    "#         loss = self.loss_fn(q, q_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33;20m[WARNING] No policy is not defined in the config file.\u001b[0m\n",
      "\u001b[33;20m[WARNING] No epsilon is not defined in the config file.\u001b[0m\n",
      "\u001b[33;20m[WARNING] No gamma is not defined in the config file.\u001b[0m\n",
      "Sequential(\n",
      "  (fc1): Linear(in_features=4, out_features=100, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=100, out_features=2, bias=True)\n",
      "  (relu2): ReLU()\n",
      ")\n",
      "\u001b[37;40m[INFO] Loading default lr. It is defined in the config file.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'lr is not defined in the config file.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Development/Room/room/common/utils.py:29\u001b[0m, in \u001b[0;36mget_param\u001b[0;34m(value, param_name, cfg, aliases)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 29\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'lr'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mDQN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Development/Room/room/agents/dqn.py:41\u001b[0m, in \u001b[0;36mDQN.__init__\u001b[0;34m(self, policy, optimizer, device, cfg, logger, lr, epsilon, gamma, *args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m=\u001b[39m get_param(gamma, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m\"\u001b[39m, cfg)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m MLP([\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m2\u001b[39m], activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# TODO: Fix this\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfigure_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_net \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_q_net \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\n",
      "File \u001b[0;32m~/Development/Room/room/agents/base.py:64\u001b[0m, in \u001b[0;36mAgent.configure_optimizer\u001b[0;34m(self, optimizer, lr)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconfigure_optimizer\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizer: Union[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer], lr: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 64\u001b[0m     lr \u001b[38;5;241m=\u001b[39m \u001b[43mget_param\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m get_optimizer(optimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n",
      "File \u001b[0;32m~/Development/Room/room/common/utils.py:31\u001b[0m, in \u001b[0;36mget_param\u001b[0;34m(value, param_name, cfg, aliases)\u001b[0m\n\u001b[1;32m     29\u001b[0m         value \u001b[38;5;241m=\u001b[39m cfg[param_name]\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not defined in the config file.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m aliases \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'lr is not defined in the config file.'"
     ]
    }
   ],
   "source": [
    "agent = DQN(policy = [], optimizer=torch.optim.Adam, device=\"cuda\", cfg={}, epsilon=0.1, gamma=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = RandomMemory(capacity=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "state = state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in trange(1000):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        action = agent.act(state[0])\n",
    "\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "    memory.add(\n",
    "        {\n",
    "            \"state\": state,\n",
    "            \"action\": action,\n",
    "            \"reward\": reward,\n",
    "            \"next_state\": next_state,\n",
    "            \"terminated\": terminated,\n",
    "            \"truncated\": truncated,\n",
    "            \"info\": info,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if not memory.is_full():\n",
    "        continue\n",
    "    else:\n",
    "        batch = memory.sample(batch_size=3)\n",
    "        agent.learn(batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if terminated.any() or truncated.any():\n",
    "            states, infos = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[0.0459, 0.0455],\n",
    "\n",
    "        [0.0459, 0.0455],\n",
    "\n",
    "        [0.0459, 0.0455]], device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.tensor([[0, 0, 0]], device='cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"action\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.gather(1, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "a40b200cef4537da6c3637769111df27f8fa364d0a5591e7f4429f061357135b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
